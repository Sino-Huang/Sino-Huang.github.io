<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>My Research (Open to Postdoc!) | Sukai Huang</title>
<meta content="" name="keywords"/>
<meta content="Sukai’s Research Details and Paper Collection" name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/biography/publications/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css" integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/biography/publications/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/biography/publications/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="My Research (Open to Postdoc!)" property="og:title"/>
<meta content="Sukai’s Research Details and Paper Collection" property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="biography" property="article:section"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/sukai_avatar.jpg" name="twitter:image"/>
<meta content="My Research (Open to Postdoc!)" name="twitter:title"/>
<meta content="Sukai’s Research Details and Paper Collection" name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Biography",
      "item": "https://sino-huang.github.io/biography/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "My Research (Open to Postdoc!)",
      "item": "https://sino-huang.github.io/biography/publications/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "My Research (Open to Postdoc!)",
  "name": "My Research (Open to Postdoc!)",
  "description": "Sukai\u0026rsquo;s Research Details and Paper Collection",
  "keywords": [
    
  ],
  "articleBody": "The Big Picture of My Research Integrating natural language into current AI system is a promising direction to democratize AI technology. Moreover, the vast knowledge embedded in natural language presents an opportunity to enhance AI-driven decision-making.\nImagine you want to instruct an AI system in Minecraft to build a house. Instead of programming a detailed set of construction rules or crafting reward functions that require expert insight, you simply tell the AI:\n“Build a two-story house with a garden, using bricks for the walls and wood for the roof.”\nA natural language (NL)-integrated sequential decision making (SDM) system shall leverage its understanding of natural language to break down this instruction into a series of actionable steps, and progressing to the desired goal you want!\nProblem (TL;DR) The current design of these natural language-integrated AI systems has significant room for improvement. For example, the algorithms often lack robustness and efficiency, which undermines the reliability of sequential decision making.\nKnow more about the problem setting (two paradigms for sequential decision-making) There are two primary paradigms for sequential decision-making: imagine you are playing Minecraft — a complex, open-ended problem-solving environment where players can build, explore, and survive. There are two ways you might approach this task.\nClick here to know more about the context\nMy research during PhD study: Improving NL-integrated SDM systems under the two paradigms 1. Model-free reinforcement learning (RL) -- (a) VLM + Language-based Reward + RL agent) Problem a: Noisy rewards from language models misguide AI agents. Solution: BiMI Reward Function (paper) Reduces false positives (e.g., rewarding irrelevant actions). Combines mutual information and thresholding for robustness. Result: Faster learning in navigation tasks (e.g., robots avoiding obstacles). 2. Model-based automated planning -- (b) LLM-Symbolic Planning Pipeline and (c) LLMs for plan generation) Problem b: LLMs hallucinate plans or require expert validation.\nSolution: Fully Automated LLM-Symbolic Pipeline (paper)\nGenerates and validates action schemas without human intervention. Resolves ambiguity by exploring multiple interpretations of language. Result: Outperforms expert-dependent methods in scalability and bias reduction. Problem c: There has been ongoing controversy about the genuine planning abilities of LLMs, with critics questioning whether their outputs reflect true reasoning or superficial statistical patterns.\nContribution: Reassessment of LLMs for end-to-end plan generation (paper)\nConducts a rigorous re-evaluation of various strategies claiming to enhance LLM reasoning in end-to-end planning, using diverse metrics for a comprehensive assessment. Found that RL promotes better generalization than supervised fine-tuning (SFT) for training LLMs to plan Future Vision Recent advances in large language models (LLMs) and vision-language models (VLMs) have sparked intense interest in improving their reasoning and decision-making capabilities. My research brought me some thoughts about how to advance this field:\nA. Extend Enhancing VLM Reward Signals for Robust Learning Beyond Passive Defense: Active Mitigation of Noisy Language Rewards While my current work (e.g., the BiMI reward function) passively filters noisy language-based rewards, future systems need active strategies to preemptively address ambiguity and noise:\nLink to Reward Hacking: Investigate how natural language reward noise interacts with RL’s inherent vulnerability to reward hacking (e.g., agents exploiting shortcuts to maximize rewards while not performing the desired behavior). Actively mitigating reward hacking issue of RL algorithms can simultaneously benefit the overall VLM reward + RL framework. Rethink Markovian Constraints: The inherent Markovian assumption of traditional RL, where rewards depend solely on the current state and action, presents a fundamental challenge when using natural language instructions. Language naturally expresses temporally extended objectives and relationships (e.g., “first, clear the table, then set the places,” or “avoid touching the hot stove while cooking”). Directly translating these instructions into Markovian rewards often leads to suboptimal learning. Research is needed into non-Markovian reward structures and corresponding RL algorithms that can naturally accommodate the long-horizon dependencies inherent in language-based instructions. This includes exploring formalisms like integrating temporal logic into RL framework and investigating how to theoretically guarantee the convergence and optimality of RL agents under such non-Markovian reward specifications. Train Robust VLMs for Reward Modeling: A critical challenge in using natural language for reward specification is mitigating the inherent ambiguity and compositionality of language, which can lead to noisy and unreliable reward signals. While our current BIMI approach acts as a form of uncertainty-aware reward modeling, further research is needed to develop more robust VLM training mechanisms. This involves several key areas: Expand to see the details Disentangling Ambiguous Instructions: Natural language instructions often contain ambiguities that make it difficult for a VLM to accurately assess alignment with agent behavior. For example, in the instruction “go to the red dot,” is the reward associated with the action (“go to”) more or the object (“red dot”) more? This ambiguity stems from the compositional nature of language, where the meaning of a phrase arises from the combination of its individual parts. Future work should focus on developing techniques to disentangle the contributions of different components of the instruction (e.g., actions, objects, attributes) to the overall reward signal. This might involve attention mechanisms or contrastive learning approaches that force the VLM to distinguish between subtly different instructions. Beyond Scalar Rewards: Expressiveness and Compositionality: Continue the discussion, due to the compositionality of language, the traditional use of a single scalar reward signal to represent the alignment between agent behavior and a language instruction may be fundamentally insufficient. As highlighted in previous work, scalar rewards struggle to capture the compositional nature of language. This raises a crucial question: should we continue to rely on scalar rewards, or explore alternative representations that better reflect the richness of language? Research is needed to: Categorize types of instructions that are (and are not) adequately expressible with scalar rewards. Develop methods to transform or decompose instructions that are not well-represented by scalar rewards into forms that are more amenable to accurate reward modeling. This could again link back to using temporal logic expression and train RL with temporal logic specifications. Multi-Objective Language Rewards: As a direct response to the limitations of scalar rewards, we may explore Multi-Objective Language Rewards. For instance, a driving instruction might have separate reward components for “speed,” “safety,” and “adherence to traffic rules.” This allows the VLM to capture more granular intent and handle potentially conflicting objectives. Techniques like PSMGD can then be further investigated to effectively train agents using language instructions. B. Addressing the Challenges of LLMs in Planning The Core Debate: Is Next-Token Prediction Fundamentally Flawed for Planning?\nA critical question looms over LLM-based planning: Does the next-token prediction paradigm—the foundation of modern LLMs – inherently limit their ability to reason sequentially? Recent work highlights two opposing perspectives:\nCritiques: Papers like \"The Pitfalls of Next-Token Prediction\" argue that autoregressive training biases LLMs toward local coherence over global reasoning, making them prone to sequential reasoning. Studies from UC Berkeley and Google suggest supervised fine-tuning (SFT) exacerbates memorization, while reinforcement learning (RL) promotes generalization for reasoning. Counterarguments: Other researchers contend that next-token prediction itself is not the bottleneck. Instead, overfitting during training – not the paradigm – causes poor planning performance. This debate underscores a pivotal challenge: How do we disentangle the limitations of next-token prediction from training-induced overfitting?\nPotential Direction 1: Mitigating Overfitting to Unlock Planning Potential Assume next-token prediction can support planning if overfitting is addressed. To test this, I propose Learn with Minimum Confidence (LMC) (related but different from Curriculum Learning). Without this strategy, models may exploit pattern shortcut to solve reasoning problems, and the reasoning ability is overshadowed by patterns matching ability.\nIntuition behind LMC Consider the double-slit experiment in Physics. If a scientist had conduct the experiments millions of times, their mindset might have overfitted to the idea that light is exclusively a wave. However, by limiting observations and gradually introducing new scenarios, physicists developed a more general understanding of the wave-particle duality character of light. Core Idea:\nOnce an LLM solves a problem instance with high confidence, no more training on that instance and its close variants. This prevent the model from exploiting shortcuts to solve the training instances Key Steps:\nFrom LLM side $\\rightarrow$ Measure LLM Confidence: Develop metrics to quantify how “certain” the model is about its solutions (e.g., entropy of output probabilities). From training data size $\\rightarrow$ Assess Instance Novelty: Compute the “surprise” of incoming training instances relative to prior data (e.g., using novelty metrics like width). Advance RL-Based Training\nOne of the theory that RL performs better than SFT is RL is better at preventing overfitting. (I really found this is akin to GANs’ advantage over VAEs). Nonetheless, further addressing overfitting in RL could lead to even greater improvements.\nSidenote: Comparative Analysis of GANs and VAEs in Mitigating Overfitting The adversarial training paradigm inherent to GANs provides built-in defenses against overfitting through three principal mechanisms:\nAdaptive Loss Surfaces: The discriminator’s evolving feedback prevents static memorization patterns. Perceptual Optimization: Prioritizing distributional matching over pixel accuracy avoids noise overfitting. Dynamic Regularization: The adversarial equilibrium maintains pressure toward generalizable feature learning. Ensemble Normalization: Train multiple reward and policy models to average out biases and stabilize learning. Replay Buffer Prioritization: Weight training instances by their novelty (e.g., prioritizing scenarios with rare task instances or under-explored plan sequences). Reward Signal Design: Our work found that Longest Contiguous Common Subsequence (LCCS)-based partial rewards help. However, this is just one approach – it remains an open problem for better reward design. Potential Direction 2: Understanding the Theoretical Limits and Capabilities of Transformers for Planning While Potential Direction 1 focuses on mitigating overfitting to improve planning performance, this direction explores the inherent theoretical capabilities and limitations of the Transformer architecture itself for planning tasks. This investigation is crucial for understanding whether the observed difficulties stem primarily from training issues (as addressed in Direction 1) or from fundamental architectural constraints.\nWe can draw inspiration from recent work, such as the UC Berkeley study analyzing Transformer limitations in sequential function composition. Similarly, we can investigate whether current LLMs, based on the Transformer architecture, can theoretically derive correct plans given any planning task specifications. Even if LLMs are found to be primarily successful with in-distribution planning tasks, further research can also examine how architectural parameters, like layer depth and width, influence this in-distribution performance. This could involve developing formal proofs or rigorous empirical analyses to characterize the planning problems solvable by Transformers of varying configurations.\nDirection 2 Variant: LLM + CoT Emulation of Planning Algorithms Above is mainly focusing on the Transformer expressiveness without Chain-of-Thought (CoT). On the other hand, the existing literature has already suggested that LLMs augmented with CoT prompting is Turing complete. That means, theoretically, LLM + CoT can simulate any planning algorithm. This implies that, in principle, LLMs can derive solutions to complex planning problems by emulating established algorithms (e.g., heuristic search, GraphPlan, partial-order planning). However, Turing completeness doesn’t guarantee efficient or robust learning. A critical open question is: which types of planning algorithms are LLMs most capable of learning and generalizing, and which are more resistant to overfitting?\nAddressing this question requires a two-pronged approach:\nEmpirical Evaluation: We must rigorously test the performance of LLMs trained to simulate various planning algorithms on a diverse range of planning benchmarks, including both in-distribution and out-of-distribution scenarios. This empirical analysis will reveal which algorithmic approaches are most amenable to learning within the LLM framework. Theoretical Analysis (RASP): Complementing empirical studies, we can leverage theoretical frameworks like RASP to analyze why certain planning algorithms are more effectively learned than others. RASP provides a formal language for describing computations within Transformers, potentially allowing us to prove properties about the learnability and generalizability of different planning strategies. Crucially, even with a theoretical understanding of Transformer capabilities, practical training remains a significant challenge. The insights and techniques developed in Potential Direction 1, such as Learn with Minimum Confidence (LMC) and advanced RL methods, are directly relevant here. For instance, even if a particular planning algorithm is theoretically learnable, effective training strategies (e.g., specialized reward signals, curriculum learning) may be necessary to achieve robust performance in practice. Therefore, this direction is deeply intertwined with the pursuit of improved training methodologies.\nLet’s team up! (*) If your lab shares these research directions – or if you’re passionate about enhancing the planning, reasoning, and decision-making capabilities of embodied agents or foundational models – I’d love to seek post-doc opportunities from you. Together, we can push the boundaries of intelligent AI systems, developing algorithms and theories that bridge language, logic, and real-world applications.\nInterested? Let’s chat: Email | LinkedIn | Google Scholar\nPublications submitted to IJCNN 2025 The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards Sukai Huang, Nir Lipovetzky and Trevor Cohn arXiv ePrint 2024 abstract paper code While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents AAAI25 Workshop LM4Plan Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation Sukai Huang, Trevor Cohn and Nir Lipovetzky Workshop on Planning in the Era of LLMs (LM4Plan @ AAAI 2025) abstract paper code The capability of Large Language Models (LLMs) to plan remains a topic of debate. Some critics argue that strategies to boost LLMs' reasoning skills are ineffective in planning tasks, while others report strong outcomes merely from training models on a planning corpus. This study reassesses recent strategies by developing an end-to-end LLM planner and employing diverse metrics for a thorough evaluation. We find that merely fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets. At the same time, we find that various strategies, including Chain-of-Thought, do enhance the probability of a plan being executable. This indicates progress towards better plan quality, despite not directly enhancing the final validity rate. Among the strategies we evaluated, reinforcement learning with our novel `Longest Contiguous Common Subsequence' reward emerged as the most effective, contributing to both plan validity and executability. Overall, our research addresses key misconceptions in the LLM-planning literature; we validate incremental progress in plan executability, although plan validity remains a challenge. Hence, future strategies should focus on both these aspects, drawing insights from our findings.\nAAAI25 Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts Sukai Huang, Nir Lipovetzky and Trevor Cohn Thirty-Ninth AAAI Conference on Artificial Intelligence abstract paper code poster Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.\npreprint A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents Sukai Huang, Nir Lipovetzky and Trevor Cohn arXiv ePrint 2023 abstract paper code Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.\nhonours thesis Angry Birds Level Generation Using Walkthrough Descriptions Sukai Huang For the degree of Bachelor of Advanced Computing (Honours) at The Australian National University abstract paper Angry Birds is a famous environment for agents to learn physical reasoning. How- ever, the deep reinforcement learning agents often underperform due to a lack of training set of game levels. To address the issue, procedural level generation is used to synthesise new Angry Birds game levels. However, the current rule-based Angry Birds procedural level generator is incapable of generating game levels that aid agents in learning physical reasoning, as it cannot guarantee the level of physical reasoning required in order to solve the generated game levels. Hence, in a new approach, we use walkthrough descriptions to generate Angry Birds game levels and train the Generative Adversarial Networks (GANs) based pro- cedural level generator by imitating the high-quality handcrafted levels. Unlike the conventional imitation approach, the proposed one is able to control the style of the generated game levels and also enhance the diversity of the game level dataset via manipulating the input walkthrough descriptions. Both qualitative and quantitative evaluations are conducted to demonstrate that the generated game levels using this method demand high level of physical reasoning to solve, just like the handcrafted game levels. Besides that, we developed a new Angry Birds walkthrough dataset called AbVat. It is a valuable dataset capable of facilitating a variety of meaningful research tasks in the domain of spatial-temporal understanding and reasoning.\n",
  "wordCount" : "3057",
  "inLanguage": "en",
  "image": "https://sino-huang.github.io/sukai_avatar.jpg","datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/biography/publications/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/biography/">Biography</a></div>
<h1 class="post-title entry-hint-parent">
      My Research (Open to Postdoc!)
    </h1>
<div class="post-meta">15 min · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/biography/publications/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header> <div class="toc">
<details open="">
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="The Big Picture of My Research" href="#the-big-picture-of-my-research">The Big Picture of My Research</a><ul>
<li>
<a aria-label="Problem (TL;DR)" href="#problem-tldr">Problem (TL;DR)</a></li>
<li>
<a aria-label="My research during PhD study: Improving NL-integrated SDM systems under the two paradigms" href="#my-research-during-phd-study-improving-nl-integrated-sdm-systems-under-the-two-paradigms">My research during PhD study: Improving NL-integrated SDM systems under the two paradigms</a></li></ul>
</li>
<li>
<a aria-label="Future Vision" href="#future-vision">Future Vision</a><ul>
<li>
<a aria-label="A. Extend Enhancing VLM Reward Signals for Robust Learning" href="#a-extend-enhancing-vlm-reward-signals-for-robust-learning">A. Extend Enhancing VLM Reward Signals for Robust Learning</a><ul>
<li>
<a aria-label="Beyond Passive Defense: Active Mitigation of Noisy Language Rewards" href="#beyond-passive-defense-active-mitigation-of-noisy-language-rewards">Beyond Passive Defense: Active Mitigation of Noisy Language Rewards</a></li></ul>
</li>
<li>
<a aria-label="B. Addressing the Challenges of LLMs in Planning" href="#b-addressing-the-challenges-of-llms-in-planning">B. Addressing the Challenges of LLMs in Planning</a><ul>
<li>
<a aria-label="Potential Direction 1: Mitigating Overfitting to Unlock Planning Potential" href="#potential-direction-1-mitigating-overfitting-to-unlock-planning-potential">Potential Direction 1: Mitigating Overfitting to Unlock Planning Potential</a></li>
<li>
<a aria-label="Potential Direction 2: Understanding the Theoretical Limits and Capabilities of Transformers for Planning" href="#potential-direction-2-understanding-the-theoretical-limits-and-capabilities-of-transformers-for-planning">Potential Direction 2: Understanding the Theoretical Limits and Capabilities of Transformers for Planning</a></li>
<li>
<a aria-label="Direction 2 Variant: LLM + CoT Emulation of Planning Algorithms" href="#direction-2-variant-llm--cot-emulation-of-planning-algorithms">Direction 2 Variant: LLM + CoT Emulation of Planning Algorithms</a></li></ul>
</li></ul>
</li>
<li>
<a aria-label="Let’s team up! (*)" href="#lets-team-up-">Let’s team up! (*)</a></li>
<li>
<a aria-label="Publications" href="#publications">Publications</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><h2 id="the-big-picture-of-my-research"><strong>The Big Picture of My Research</strong><a aria-hidden="true" class="anchor" hidden="" href="#the-big-picture-of-my-research">#</a></h2>
<p>Integrating natural language into current AI system is a promising direction to <strong>democratize AI technology</strong>. Moreover, the <strong>vast knowledge embedded</strong> in natural language presents an opportunity to enhance AI-driven decision-making.</p>
<blockquote>
<p>Imagine you want to instruct an AI system in Minecraft to build a house. Instead of programming a detailed set of construction rules or crafting reward functions that require expert insight, you simply tell the AI:</p>
<p>  “Build a two-story house with a garden, using bricks for the walls and wood for the roof.”</p>
<p>A natural language (NL)-integrated sequential decision making (SDM) system shall leverage its understanding of natural language to break down this instruction into a series of actionable steps, and progressing to the desired goal you want!</p>
</blockquote>
<h3 id="problem-tldr"><strong>Problem (TL;DR)</strong><a aria-hidden="true" class="anchor" hidden="" href="#problem-tldr">#</a></h3>
<p>The current design of these natural language-integrated AI systems has significant room for improvement. For example, the algorithms often lack robustness and efficiency, which undermines the reliability of sequential decision making.</p>
<details>
<summary>Know more about the problem setting (two paradigms for sequential decision-making)</summary>
<div><p><img alt="illustration of two paradigms part A" loading="lazy" src="./image-assets/1_2_twotypeoflrs_overall_part_a.png"/></p>
<p><img alt="illustration of two paradigms part B" loading="lazy" src="./image-assets/1_2_twotypeoflrs_overall_part_b.png"/></p>
<p>There are two primary paradigms for sequential decision-making: imagine you are playing Minecraft — a complex, open-ended problem-solving environment where players can build, explore, and survive. There are two ways you might approach this task.</p>
<p><a href="../detailed-background">Click here to know more about the context</a></p>
</div>
</details>
<h3 id="my-research-during-phd-study-improving-nl-integrated-sdm-systems-under-the-two-paradigms"><strong>My research during PhD study: Improving NL-integrated SDM systems under the two paradigms</strong><a aria-hidden="true" class="anchor" hidden="" href="#my-research-during-phd-study-improving-nl-integrated-sdm-systems-under-the-two-paradigms">#</a></h3>
<details>
<summary>1. Model-free reinforcement learning (RL) -- (a) VLM + Language-based Reward + RL agent)</summary>
<div><p><img alt="language reward model illustration" loading="lazy" src="image-assets/1_2_twotypeoflrs_overall.png"/></p>
<ul>
<li><em>Problem a</em>: Noisy rewards from language models misguide AI agents.</li>
<li><em>Solution</em>: <strong>BiMI Reward Function</strong> (<a href="https://arxiv.org/abs/2409.15922">paper</a>)
<ul>
<li>Reduces false positives (e.g., rewarding irrelevant actions).</li>
<li>Combines mutual information and thresholding for robustness.</li>
<li><em>Result</em>: Faster learning in navigation tasks (e.g., robots avoiding obstacles).</li>
</ul>
</li>
</ul></div>
</details>
<details>
<summary>2. Model-based automated planning -- (b) LLM-Symbolic Planning Pipeline and (c) LLMs for plan generation)</summary>
<div><p><img alt="automated planning senario" loading="lazy" src="image-assets/1_3_llm_with_planning.png"/></p>
<ul>
<li>
<p><em>Problem b</em>: LLMs hallucinate plans or require expert validation.</p>
</li>
<li>
<p><em>Solution</em>: <strong>Fully Automated LLM-Symbolic Pipeline</strong> (<a href="https://arxiv.org/abs/2409.15915">paper</a>)</p>
<ul>
<li>Generates and validates action schemas without human intervention.</li>
<li>Resolves ambiguity by exploring multiple interpretations of language.</li>
<li><em>Result</em>: Outperforms expert-dependent methods in scalability and bias reduction.</li>
</ul>
</li>
<li>
<p><em>Problem c</em>: There has been ongoing controversy about the genuine planning abilities of LLMs, with critics questioning whether their outputs reflect true reasoning or superficial statistical patterns.</p>
</li>
<li>
<p><em>Contribution</em>: <strong>Reassessment of LLMs for end-to-end plan generation</strong> (<a href="https://arxiv.org/abs/2412.10675">paper</a>)</p>
<ul>
<li>Conducts a rigorous re-evaluation of <strong>various</strong> strategies claiming to enhance LLM reasoning in  end-to-end planning, using diverse metrics for a comprehensive assessment.
<ul>
<li>Found that RL promotes better generalization than supervised fine-tuning (SFT) for training LLMs to plan</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</details>
<h2 id="future-vision"><strong>Future Vision</strong><a aria-hidden="true" class="anchor" hidden="" href="#future-vision">#</a></h2>
<p>Recent advances in large language models (LLMs) and vision-language  models (VLMs) have sparked intense interest in improving their reasoning and decision-making capabilities. My research brought me some thoughts about how to advance this field:</p>
<h3 id="a-extend-enhancing-vlm-reward-signals-for-robust-learning">A. Extend Enhancing VLM Reward Signals for Robust Learning<a aria-hidden="true" class="anchor" hidden="" href="#a-extend-enhancing-vlm-reward-signals-for-robust-learning">#</a></h3>
<h4 id="beyond-passive-defense-active-mitigation-of-noisy-language-rewards"><strong>Beyond Passive Defense: Active Mitigation of Noisy Language Rewards</strong><a aria-hidden="true" class="anchor" hidden="" href="#beyond-passive-defense-active-mitigation-of-noisy-language-rewards">#</a></h4>
<p>While my current work (e.g., the <strong>BiMI</strong> reward function) passively filters noisy language-based rewards, future systems need <em>active</em> strategies to preemptively address ambiguity and noise:</p>
<ul>
<li><strong>Link to Reward Hacking</strong>: Investigate how natural language reward noise interacts with RL’s inherent vulnerability to <a href="https://lilianweng.github.io/posts/2024-11-28-reward-hacking/">reward hacking</a> (e.g., agents exploiting shortcuts to maximize rewards while not performing the desired behavior). Actively mitigating <em><strong>reward hacking issue</strong></em> of RL algorithms can simultaneously benefit the overall VLM reward + RL framework.</li>
<li><strong>Rethink Markovian Constraints</strong>: The inherent <strong>Markovian assumption</strong> of traditional RL, where rewards depend solely on the current state and action, presents a fundamental challenge when using natural language instructions.  Language naturally  expresses <em>temporally extended objectives and relationships</em> (e.g., “first, clear the table, <em>then</em> set the places,” or “avoid touching the hot stove <em>while</em> cooking”). Directly translating these instructions into Markovian rewards often leads to suboptimal learning.  Research is needed into non-Markovian reward structures and corresponding RL algorithms that can <em><strong>naturally accommodate the long-horizon dependencies inherent in language-based instructions</strong></em>. This includes exploring formalisms like <a href="https://arxiv.org/abs/2402.17217">integrating temporal logic into RL framework</a> and investigating how to theoretically guarantee the convergence and optimality of RL agents under such non-Markovian reward specifications.</li>
<li><strong>Train Robust VLMs for Reward Modeling</strong>: A critical challenge in using natural language for reward specification  is mitigating the inherent ambiguity and compositionality of language,  which can lead to noisy and unreliable reward signals. While our current <strong>BIMI</strong> approach acts as a form of  uncertainty-aware reward modeling, further research is needed to develop more robust VLM training mechanisms. This involves several key areas:
<ul>
<li><details>
<summary>Expand to see the details</summary>
<div><ol>
<li><strong>Disentangling Ambiguous Instructions:</strong> Natural  language instructions often contain ambiguities that make it difficult  for a VLM to accurately assess alignment with agent behavior. For  example, in the instruction “go to the red dot,” is the reward associated with the action (“go to”) more or the object (“red dot”) more?  This ambiguity stems from the compositional nature of language, where the meaning of a phrase arises from the combination of its individual parts.  Future work should focus on developing techniques to disentangle the contributions of different components of the instruction (e.g., actions, objects, attributes) to the overall reward signal.  This might involve attention mechanisms or contrastive learning approaches that force the VLM to distinguish between subtly different instructions.</li>
<li><strong>Beyond Scalar Rewards: Expressiveness and Compositionality:</strong>  Continue the discussion, due to the compositionality of language, the traditional use of a single scalar reward signal to represent the alignment between agent behavior and a language instruction may be <strong>fundamentally insufficient.</strong> As highlighted in <a href="https://openreview.net/forum?id=NjOoxFRZA4&amp;noteId=niZsZfTPPt">previous work</a>, scalar rewards struggle to capture the compositional nature of language.  This raises a crucial question: <em>should we continue to rely on scalar rewards, or explore alternative  representations that better reflect the richness of language?</em>  Research is needed to:
<ul>
<li>Categorize types of instructions that are (and are not) adequately <strong>expressible</strong> with scalar rewards.</li>
<li>Develop methods to transform or decompose instructions that are not well-represented by scalar rewards into forms that are more amenable to  accurate reward modeling. This could again link back to using temporal logic expression and train RL with temporal logic specifications.</li>
</ul>
</li>
<li><strong>Multi-Objective Language Rewards:</strong> As a direct response to the limitations of scalar rewards, we may explore <strong>Multi-Objective Language Rewards</strong>.  For instance, a driving instruction might have separate reward components for “speed,”  “safety,” and “adherence to traffic rules.”  This allows the VLM to capture more granular intent and handle potentially conflicting objectives. Techniques like <a href="https://arxiv.org/abs/2412.10961">PSMGD</a> can then be further investigated to effectively train agents using language instructions.</li>
</ol>
</div>
</details>
</li>
</ul>
</li>
</ul>
<h3 id="b-addressing-the-challenges-of-llms-in-planning">B. Addressing the Challenges of LLMs in Planning<a aria-hidden="true" class="anchor" hidden="" href="#b-addressing-the-challenges-of-llms-in-planning">#</a></h3>
<p><strong>The Core Debate: Is Next-Token Prediction Fundamentally Flawed for Planning?</strong></p>
<p>A critical question looms over LLM-based planning: <em>Does the next-token prediction paradigm—the foundation of modern LLMs – inherently limit their ability to reason sequentially?</em> Recent work highlights two opposing perspectives:</p>
<ol>
<li><strong>Critiques</strong>:
<ul>
<li>Papers like <em>"<a href="https://arxiv.org/abs/2403.06963">The Pitfalls of Next-Token Prediction</a>"</em> argue that autoregressive training biases LLMs toward local coherence over global reasoning, making them prone to sequential reasoning.</li>
<li>Studies from <a href="https://arxiv.org/abs/2501.17161">UC Berkeley and Google</a> suggest <strong>supervised fine-tuning (SFT)</strong> exacerbates memorization, while <strong>reinforcement learning (RL)</strong> promotes generalization for reasoning.</li>
</ul>
</li>
<li><strong>Counterarguments</strong>:
<ul>
<li>Other <a href="https://arxiv.org/abs/2410.13779">researchers</a> contend that next-token prediction itself is not the bottleneck. Instead, <strong>overfitting during training</strong> – not the paradigm – causes poor planning performance.</li>
</ul>
</li>
</ol>
<p>This debate underscores a pivotal challenge: <em>How do we disentangle the limitations of next-token prediction from training-induced overfitting?</em></p>
<h4 id="potential-direction-1-mitigating-overfitting-to-unlock-planning-potential"><strong>Potential Direction 1: Mitigating Overfitting to Unlock Planning Potential</strong><a aria-hidden="true" class="anchor" hidden="" href="#potential-direction-1-mitigating-overfitting-to-unlock-planning-potential">#</a></h4>
<p>Assume next-token prediction <em>can</em> support planning if overfitting is addressed. To test this, I propose <strong>Learn with Minimum Confidence (LMC)</strong> (related but different from <em><a href="https://arxiv.org/abs/2101.10382">Curriculum Learning</a></em>). Without this strategy, models may exploit pattern shortcut to solve reasoning problems, and the reasoning ability is overshadowed by patterns matching ability.</p>
<details>
<summary>Intuition behind LMC</summary>
<div>Consider the <a href="https://en.wikipedia.org/wiki/Double-slit_experiment">double-slit experiment</a> in Physics. If a scientist had conduct the experiments millions of times, their mindset might have overfitted to the idea that light is exclusively a wave. However, by limiting observations and gradually introducing new scenarios, physicists developed a more general understanding of the wave-particle duality character of light.</div>
</details>
<p><strong>Core Idea</strong>:</p>
<ul>
<li>Once an LLM solves a problem instance with high confidence, <strong>no more training</strong> on that instance and its close variants.</li>
<li>This prevent the model from exploiting shortcuts to solve the training instances</li>
</ul>
<p><strong>Key Steps</strong>:</p>
<ol>
<li>From LLM side $\rightarrow$ <strong>Measure LLM Confidence</strong>: Develop metrics to quantify how “certain” the model is about its solutions (e.g., entropy of output probabilities).</li>
<li>From training data size $\rightarrow$ <strong>Assess Instance Novelty</strong>: Compute the “surprise” of incoming training instances relative to prior data (e.g., using novelty metrics like <em><a href="https://nirlipo.github.io/project/width-based-planning/">width</a></em>).</li>
</ol>
<p><strong>Advance RL-Based Training</strong></p>
<p>One of the theory that RL performs better than SFT is RL is better at preventing overfitting. (I really found this is akin to GANs’ advantage over VAEs). Nonetheless, further addressing overfitting in RL could lead to even greater improvements.</p>
<details>
<summary>Sidenote: Comparative Analysis of GANs and VAEs in Mitigating Overfitting</summary>
<div><p>The adversarial training paradigm inherent to GANs provides built-in defenses against overfitting through three principal mechanisms:</p>
<ol>
<li><strong>Adaptive Loss Surfaces</strong>: The discriminator’s evolving feedback prevents static memorization patterns.</li>
<li><strong>Perceptual Optimization</strong>: Prioritizing distributional matching over pixel accuracy avoids noise overfitting.</li>
<li><strong>Dynamic Regularization</strong>: The adversarial equilibrium maintains pressure toward generalizable feature learning.</li>
</ol></div>
</details>
<ol>
<li><strong>Ensemble Normalization</strong>:
<ul>
<li>Train multiple reward and policy models to average out biases and stabilize learning.</li>
</ul>
</li>
<li><strong>Replay Buffer Prioritization</strong>:
<ul>
<li>Weight training instances by their novelty (e.g., prioritizing scenarios with rare task instances or under-explored plan sequences).</li>
</ul>
</li>
<li><strong>Reward Signal Design</strong>:
<ul>
<li>Our work found that <strong>Longest Contiguous Common Subsequence (LCCS)</strong>-based partial rewards help. However, this is just one approach – it remains an open problem for better reward design.</li>
</ul>
</li>
</ol>
<h4 id="potential-direction-2-understanding-the-theoretical-limits-and-capabilities-of-transformers-for-planning"><strong>Potential Direction 2: Understanding the Theoretical Limits and Capabilities of Transformers for Planning</strong><a aria-hidden="true" class="anchor" hidden="" href="#potential-direction-2-understanding-the-theoretical-limits-and-capabilities-of-transformers-for-planning">#</a></h4>
<p>While Potential Direction 1 focuses on mitigating overfitting to improve planning performance, this direction explores the inherent <em>theoretical</em> capabilities and limitations of the Transformer architecture itself for planning tasks. This investigation is crucial for understanding whether the observed difficulties stem primarily from training issues (as  addressed in Direction 1) or from fundamental architectural constraints.</p>
<p>We can draw inspiration from recent work, such as the <a href="https://arxiv.org/abs/2412.02975">UC Berkeley study</a> analyzing Transformer limitations in sequential function composition.  Similarly, we can investigate whether current LLMs, based on the Transformer architecture, can theoretically derive correct plans given any planning task specifications. Even if LLMs are found to be primarily successful with <em>in-distribution</em> planning tasks, further research can also examine how architectural parameters, like layer depth and width, influence this in-distribution performance. This could involve developing formal proofs or rigorous empirical analyses to characterize the planning problems solvable by Transformers of varying configurations.</p>
<h4 id="direction-2-variant-llm--cot-emulation-of-planning-algorithms"><strong>Direction 2 Variant: LLM + CoT Emulation of Planning Algorithms</strong><a aria-hidden="true" class="anchor" hidden="" href="#direction-2-variant-llm--cot-emulation-of-planning-algorithms">#</a></h4>
<p>Above is mainly focusing on the Transformer expressiveness without Chain-of-Thought (CoT). On the other hand, the existing literature has already suggested that LLMs augmented with <a href="https://openreview.net/forum?id=AS8SPTyBgw">CoT prompting is Turing complete</a>. That means, theoretically, LLM + CoT can simulate any planning algorithm. This implies that, in principle, LLMs <em>can</em> derive solutions to complex planning problems by emulating established algorithms (e.g., heuristic search, GraphPlan, partial-order planning).  However, Turing completeness doesn’t guarantee <em>efficient or robust learning</em>.  A critical open question is: <em>which types of planning algorithms are LLMs most capable of learning and  generalizing, and which are more resistant to overfitting?</em></p>
<p>Addressing this question requires a two-pronged approach:</p>
<ol>
<li><strong>Empirical Evaluation:</strong> We must rigorously test the performance of LLMs trained to simulate various planning algorithms on a diverse range of planning benchmarks, including both in-distribution and out-of-distribution scenarios. This empirical analysis will reveal which algorithmic approaches are most amenable to learning within the LLM framework.</li>
<li><strong>Theoretical Analysis (RASP):</strong>  Complementing empirical studies, we can leverage theoretical frameworks like <a href="https://arxiv.org/abs/2310.16028">RASP</a> to analyze <em>why</em> certain planning algorithms are more effectively learned than others. RASP provides a formal language for describing computations within Transformers, potentially allowing us to prove properties about the  learnability and generalizability of different planning strategies.</li>
</ol>
<p>Crucially, even with a theoretical understanding of Transformer capabilities, <em>practical training remains a significant challenge</em>.  The insights and techniques developed in Potential Direction 1, such as Learn with Minimum Confidence (LMC) and advanced RL methods, are directly relevant here. For instance, even if a particular planning algorithm is theoretically learnable, effective training strategies  (e.g., specialized reward signals, curriculum learning) may be necessary to achieve robust performance in practice. Therefore, this direction is deeply intertwined with the pursuit of improved training methodologies.</p>
<h2 id="lets-team-up-"><strong>Let’s team up! (*)</strong><a aria-hidden="true" class="anchor" hidden="" href="#lets-team-up-">#</a></h2>
<p>If your lab shares these <a href="/biography/publications/#future-vision">research directions</a> – or if you’re passionate about enhancing the planning, reasoning, and decision-making capabilities of embodied agents or foundational models – I’d love to seek post-doc opportunities from you. Together, we can push the boundaries of intelligent AI systems, developing algorithms and theories that bridge language, logic, and real-world applications.</p>
<p><em>Interested? Let’s chat:</em> <a href="mailto:sukaih@student.unimelb.edu.au">Email</a> | <a href="https://au.linkedin.com/in/sukai-huang-683368169">LinkedIn</a> | <a href="https://scholar.google.com/citations?user=oE9JP1IAAAAJ">Google Scholar</a></p>
<h2 id="publications"><strong>Publications</strong><a aria-hidden="true" class="anchor" hidden="" href="#publications">#</a></h2>
<style>
    .badge {
        display: inline-block;
        padding: .25em .4em;
        font-size: 75%;
        font-weight: 700;
        line-height: 1;
        text-align: center;
        white-space: nowrap;
        vertical-align: baseline;
        border-radius: .25rem;
        transition: color .15s ease-in-out, background-color .15s ease-in-out, border-color .15s ease-in-out, box-shadow .15s ease-in-out;
    }

    .badge {
        color: rgb(81, 194, 29) !important;
        border-radius: .125rem;
        -webkit-box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.16), 0 2px 10px 0 rgba(0, 0, 0, 0.12);
        box-shadow: 0 2px 5px 0 rgba(0, 0, 0, 0.16), 0 2px 10px 0 rgba(0, 0, 0, 0.12);
    }

    .publications ol.bibliography li .title {
        font-weight: bolder;
    }

    .publications ol.bibliography {
        list-style: none;
        padding: 0;
        margin-top: 0;
    }

    .publications ol.bibliography li {
        margin-bottom: 1rem;
    }

    .row {
        display: -ms-flexbox;
        display: flex;
        -ms-flex-wrap: wrap;
        flex-wrap: wrap;
        margin-right: -15px;
        margin-left: 0px;
    }

    .publications ol.bibliography li .links a.btn {
	color: #111;
	border: 1px solid #111;
	padding-left: 1rem;
	padding-right: 1rem;
	padding-top: 0.25rem;
	padding-bottom: 0.25rem;
    margin-top: 0.55rem;
    font-size: small;
}

.publications ol.bibliography li .periodical {
	padding-bottom: 0.15rem;
    margin: 0.15rem;
}

    .publications ol.bibliography li .abbr {
        height: 2rem;
        margin-bottom: 0.5rem;
    }

    .publications ol.bibliography li .links a.btn:hover {
        color: #579968;
        border-color: #579968;
    }

    .publications ol.bibliography li div.abstract.hidden.open {
	border-color: #828282;
}

.publications ol.bibliography li div.abstract.hidden {
	border: dashed 1px white;
}
.publications ol.bibliography li .hidden.open {
	max-height: 100em;
	-webkit-transition: 0.15s ease;
	-moz-transition: 0.15s ease;
	-ms-transition: 0.15s ease;
	-o-transition: 0.15s ease;
	transition: all 0.15s ease;
}
.publications ol.bibliography li .hidden {
	font-size: 0.875rem;
	max-height: 0px;
	overflow: hidden;
	text-align: justify;
	-webkit-transition: 0.15s ease;
	-moz-transition: 0.15s ease;
	-ms-transition: 0.15s ease;
	-o-transition: 0.15s ease;
	transition: all 0.15s ease;
}
</style>
<script crossorigin="anonymous" integrity="sha256-2Pmvv0kuTBOenSvLm6bvfBSSHrUJ+3A7x6P5Ebd07/g=" src="https://code.jquery.com/jquery-3.7.0.min.js"></script>
<script>
    $(document).ready(function() {
    $('a.abstract').click(function() {
        $(this).parent().parent().find(".abstract.hidden").toggleClass('open');
    });
    $('a.bibtex').click(function() {
        $(this).parent().parent().find(".bibtex.hidden").toggleClass('open');
    });
});

</script>
<div class="publications">
<ol class="bibliography">
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">submitted to IJCNN 2025</abbr>
</div>
<div class="col-sm-8">
<div class="title">The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards</div>
<div class="author">
                         Sukai Huang, Nir Lipovetzky and Trevor Cohn
 
                     </div>
<div class="periodical">
<em>arXiv ePrint 2024</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2409.15922" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://www.shorturl.at/VsH70" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>While Vision-Language Models (VLMs) are increasingly used to generate reward signals for training embodied agents to follow instructions, our research reveals that agents guided by VLM rewards often underperform compared to those employing only intrinsic (exploration-driven) rewards, contradicting expectations set by recent work. We hypothesize that false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives. Our analysis confirms this hypothesis, revealing that the widely used cosine similarity metric is prone to false positive reward estimates. To address this, we introduce BiMI ({Bi}nary {M}utual {I}nformation), a novel reward function designed to mitigate noise. BiMI significantly enhances learning efficiency across diverse and challenging embodied navigation environments. Our findings offer a nuanced understanding of how different types of reward noise impact agent learning and highlight the importance of addressing multimodal reward signal noise when training embodied agents </p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">AAAI25 Workshop LM4Plan</abbr>
</div>
<div class="col-sm-8">
<div class="title">Chasing Progress, Not Perfection: Revisiting Strategies for End-to-End LLM Plan Generation</div>
<div class="author">
                        Sukai Huang, Trevor Cohn and Nir Lipovetzky

                    </div>
<div class="periodical">
<em>Workshop on Planning in the Era of LLMs (LM4Plan @ AAAI 2025)</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2412.10675" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://anonymous.4open.science/r/official-misconcept-lm-plan-gen-D34B" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>The capability of Large Language Models (LLMs) to plan remains a topic  of debate. Some critics argue that strategies to boost LLMs' reasoning  skills are ineffective in planning tasks, while others report strong  outcomes merely from training models on a planning corpus. This study  reassesses recent strategies by developing an end-to-end LLM planner and employing diverse metrics for a thorough evaluation. We find that  merely fine-tuning LLMs on a corpus of planning instances does not lead  to robust planning skills, as indicated by poor performance on  out-of-distribution test sets. At the same time, we find that various  strategies, including Chain-of-Thought, do enhance the probability of a  plan being executable. This indicates progress towards better plan  quality, despite not directly enhancing the final validity rate. Among  the strategies we evaluated, reinforcement learning with our novel  `Longest Contiguous Common Subsequence' reward emerged as the most  effective, contributing to both plan validity and executability.  Overall, our research addresses key misconceptions in the LLM-planning  literature; we validate incremental progress in plan executability,  although plan validity remains a challenge. Hence, future strategies  should focus on both these aspects, drawing insights from our findings.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">AAAI25</abbr>
</div>
<div class="col-sm-8">
<div class="title">Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts</div>
<div class="author">
                         Sukai Huang, Nir Lipovetzky and Trevor Cohn
 
                     </div>
<div class="periodical">
<em>Thirty-Ninth AAAI Conference on Artificial Intelligence</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2409.15915" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://github.com/Sino-Huang/Official-LLM-Symbolic-Planning-without-Experts" role="button" target="_blank">code</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="./AAAI25_Planning_in_the_Dark_poster.pdf" role="button" target="_blank">poster</a>
</div>
<div class="abstract hidden">
<p>Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">preprint</abbr>
</div>
<div class="col-sm-8">
<div class="title">A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents</div>
<div class="author">
                        Sukai Huang, Nir Lipovetzky and Trevor Cohn

                    </div>
<div class="periodical">
<em>arXiv ePrint 2023</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://arxiv.org/abs/2305.16621" role="button" target="_blank">paper</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="https://github.com/Sino-Huang/Brittleness_of_LRS" role="button" target="_blank">code</a>
</div>
<div class="abstract hidden">
<p>Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.</p>
</div>
</div>
</div>
</li>
<li>
<div class="row">
<div class="col-sm-2 abbr">
<abbr class="badge">honours thesis</abbr>
</div>
<div class="col-sm-8">
<div class="title">Angry Birds Level Generation Using Walkthrough Descriptions</div>
<div class="author">
                        Sukai Huang

                    </div>
<div class="periodical">
<em>For the degree of Bachelor of Advanced Computing (Honours) at The Australian National University</em>
<br/>
</div>
<div class="links">
<a class="abstract btn btn-sm z-depth-0 waves-effect waves-light" role="button">abstract</a>
<a class="btn btn-sm z-depth-0 waves-effect waves-light" href="./honours_thesis.pdf" role="button" target="_blank">paper</a>
</div>
<div class="abstract hidden">
<p>Angry Birds is a famous environment for agents to learn physical reasoning. How- ever, the deep reinforcement learning agents often underperform due to a lack of training set of game levels. To address the issue, procedural level generation is used to synthesise new Angry Birds game levels. However, the current rule-based Angry Birds procedural level generator is incapable of generating game levels that aid agents in learning physical reasoning, as it cannot guarantee the level of physical reasoning required in order to solve the generated game levels. Hence, in a new approach, we use walkthrough descriptions to generate Angry Birds game levels and train the Generative Adversarial Networks (GANs) based pro- cedural level generator by imitating the high-quality handcrafted levels. Unlike the conventional imitation approach, the proposed one is able to control the style of the generated game levels and also enhance the diversity of the game level dataset via manipulating the input walkthrough descriptions. Both qualitative and quantitative evaluations are conducted to demonstrate that the generated game levels using this method demand high level of physical reasoning to solve, just like the handcrafted game levels. Besides that, we developed a new Angry Birds walkthrough dataset called AbVat. It is a valuable dataset capable of facilitating a variety of meaningful research tasks in the domain of spatial-temporal understanding and reasoning.</p>
</div>
</div>
</div>
</li>
</ol>
</div>
</div>
<footer class="post-footer">
<ul class="post-tags">
</ul>
<ul class="share-buttons">
<li>
<a aria-label="share My Research (Open to Postdoc!) on x" href="https://x.com/intent/tweet/?text=My%20Research%20%28Open%20to%20Postdoc%21%29&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;hashtags=" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;title=My%20Research%20%28Open%20to%20Postdoc%21%29&amp;summary=My%20Research%20%28Open%20to%20Postdoc%21%29&amp;source=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f&amp;title=My%20Research%20%28Open%20to%20Postdoc%21%29" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on whatsapp" href="https://api.whatsapp.com/send?text=My%20Research%20%28Open%20to%20Postdoc%21%29%20-%20https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on telegram" href="https://telegram.me/share/url?text=My%20Research%20%28Open%20to%20Postdoc%21%29&amp;url=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share My Research (Open to Postdoc!) on ycombinator" href="https://news.ycombinator.com/submitlink?t=My%20Research%20%28Open%20to%20Postdoc%21%29&amp;u=https%3a%2f%2fsino-huang.github.io%2fbiography%2fpublications%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
