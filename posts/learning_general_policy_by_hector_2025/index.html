<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>Learning General Policies Through Sketch @ Hector Geffner | Sukai Huang</title>
<meta content="knowledge representation, sketch" name="keywords"/>
<meta content="I will list some important literatures about the topic of learning general policies through sketches
The research is initiated by Blai Bonet and Hector Geffner
The high level goal of the research is as follows

[!IMPORTANT]
The construction of reusable knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large." name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.216db197d28df88a47bfb89498be748381c9d960a20db4b7d59e51e8b91f60b6.css" integrity="sha256-IW2xl9KN+IpHv7iUmL50g4HJ2WCiDbS31Z5R6LkfYLY=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="Learning General Policies Through Sketch @ Hector Geffner" property="og:title"/>
<meta content="I will list some important literatures about the topic of learning general policies through sketches
The research is initiated by Blai Bonet and Hector Geffner
The high level goal of the research is as follows
[!IMPORTANT]
The construction of reusable knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large." property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2025-03-04T12:07:13+11:00" property="article:published_time"/>
<meta content="2025-03-04T12:07:13+11:00" property="article:modified_time"/>
<meta content="Knowledge Representation" property="article:tag"/>
<meta content="Sketch" property="article:tag"/>
<meta content="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/image-assets/cover.png" name="twitter:image"/>
<meta content="Learning General Policies Through Sketch @ Hector Geffner" name="twitter:title"/>
<meta content="I will list some important literatures about the topic of learning general policies through sketches
The research is initiated by Blai Bonet and Hector Geffner
The high level goal of the research is as follows

[!IMPORTANT]
The construction of reusable knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large." name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Learning General Policies Through Sketch @ Hector Geffner",
      "item": "https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Learning General Policies Through Sketch @ Hector Geffner",
  "name": "Learning General Policies Through Sketch @ Hector Geffner",
  "description": "I will list some important literatures about the topic of learning general policies through sketches\nThe research is initiated by Blai Bonet and Hector Geffner\nThe high level goal of the research is as follows\n[!IMPORTANT]\nThe construction of reusable knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large.\n",
  "keywords": [
    "knowledge representation", "sketch"
  ],
  "articleBody": "I will list some important literatures about the topic of learning general policies through sketches\nThe research is initiated by Blai Bonet and Hector Geffner\nThe high level goal of the research is as follows\n[!IMPORTANT]\nThe construction of reusable knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large.\nFor addressing this challenge, new ideas and methods are required that build on those of planning, deep and reinforcement learning, logic and knowledge representation, and combinatorial optimization. Our approach is a form of top-down representation learning based on a clear separation and characterization of what is to be learned from how (Video). The research methodology that is common in deep learning, which focuses mainly on relative performance is not good enough as it does not give us a crisp understanding. At the same time, we cannot afford not to use deep learning, namely, the optimization of parametric functions (neural nets) by means of stochastic gradient descent, because deep learning represents a very useful, versatile, and effective class of solvers.\nAn intro: ICAPS 2024 Keynote: Hector Geffner on “Learning Representations to Act and Plan” Learning Sketch Decompositions in Planning via Deep Reinforcement Learning https://arxiv.org/abs/2412.08574\nConclusion quote: We have demonstrated that DRL methods can effectively learn the common subgoal structures of entire collections of planning problems, enabling them to be solved efficiently through greedy sequences of IW(k) searches.\nSymmetries and Expressive Requirements for Learning General Policies https://arxiv.org/pdf/2409.15892\nAbstract: State symmetries play an important role in planning and generalized planning. In the first case, state symmetries can be used to reduce the size of the search; in the second, to reduce the size of the training set. In the case of general planning, however, it is also critical to distinguish non-symmetric states, i.e., states that represent non-isomorphic relational structures. However, while the language of first-order logic distinguishes non-symmetric states, the languages and architectures used to represent and learn general policies do not. In particular, recent approaches for learning general policies use state features derived from description logics or learned via graph neural networks (GNNs) that are known to be limited by the expressive power of C2, first-order logic with two variables and counting. In this work, we address the problem of detecting symmetries in planning and generalized planning and use the results to assess the expressive requirements for learning general policies over various planning domains. For this, we map planning states to plain graphs, run off-the-shelf algorithms to determine whether two states are isomorphic with respect to the goal, and run coloring algorithms to determine if C2 features computed logically or via GNNs distinguish non-isomorphic states. Symmetry detection results in more effective learning, while the failure to detect non-symmetries prevents general policies from being learned at all in certain domains\nOn Policy Reuse: An Expressive Language for Representing and Executing General Policies that Call Other Policies https://openreview.net/forum?id=TSl0tWPiXT\nhttps://ojs.aaai.org/index.php/ICAPS/article/view/31458/33618\nIt provides a types of language to express the general policies (which is a collection of rules)\nLearning General Policies with Policy Gradient Methods https://proceedings.kr.org/2023/63/kr2023-0063-stahlberg-et-al.pdf\nThe learning of general policy is different from standard policy gradient learning as the general policy has different semantic than the standard policy\nThe long dicussion of the search: https://arxiv.org/pdf/2311.05490 ",
  "wordCount" : "555",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/image-assets/cover.png","datePublished": "2025-03-04T12:07:13+11:00",
  "dateModified": "2025-03-04T12:07:13+11:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      Learning General Policies Through Sketch @ Hector Geffner
    </h1>
<div class="post-meta"><span title="2025-03-04 12:07:13 +1100 AEDT">March 4, 2025</span> · 3 min · 555 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/posts/learning_general_policy_by_hector_2025/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/learning_general_policy_by_hector_2025/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul>
<li>
<a aria-label="An intro: ICAPS 2024 Keynote: Hector Geffner on “Learning Representations to Act and Plan”" href="#an-intro-icaps-2024-keynote-hector-geffner-on-learning-representations-to-act-and-plan">An intro: ICAPS 2024 Keynote: Hector Geffner on “Learning Representations to Act and Plan”</a></li>
<li>
<a aria-label="Learning Sketch Decompositions in Planning via Deep Reinforcement Learning" href="#learning-sketch-decompositions-in-planning-via-deep-reinforcement-learning">Learning Sketch Decompositions in Planning via Deep Reinforcement Learning</a></li>
<li>
<a aria-label="Symmetries and Expressive Requirements for Learning General Policies" href="#symmetries-and-expressive-requirements-for-learning-general-policies">Symmetries and Expressive Requirements for Learning General Policies</a></li>
<li>
<a aria-label="On Policy Reuse: An Expressive Language for Representing and Executing General Policies that Call Other Policies" href="#on-policy-reuse-an-expressive-language-for-representing-and-executing-general-policies-that-call-other-policies">On Policy Reuse: An Expressive Language for Representing and Executing General Policies that Call Other Policies</a></li>
<li>
<a aria-label="Learning General Policies with Policy Gradient Methods" href="#learning-general-policies-with-policy-gradient-methods">Learning General Policies with Policy Gradient Methods</a></li>
<li>
<a aria-label="The long dicussion of the search: https://arxiv.org/pdf/2311.05490" href="#the-long-dicussion-of-the-search">The long dicussion of the search: https://arxiv.org/pdf/2311.05490</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>I will list some important literatures about the topic of <em>learning general policies through sketches</em></p>
<p>The research is initiated by Blai Bonet and Hector Geffner</p>
<p>The high level goal of the research is as follows</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>The construction of <strong>reusable</strong> knowledge (transfer learning) is a central concern in (deep) reinforcement learning, but the semantic and conceptual gap between the low level techniques that are used, and the high-level representations that are required, is too large.</p>
<p>For addressing this challenge, new ideas and methods are required that build on those of  planning, deep and reinforcement learning, logic and knowledge representation, and combinatorial optimization. Our approach is a form of <strong>top-down representation learning based on a clear separation and characterization of what is to be learned from how</strong> (<a href="https://www.youtube.com/watch?v=EZ2bb-ZiVsE&amp;ab_channel=ICAPS">Video</a>). The research methodology that is common in deep learning, which focuses mainly on relative performance is not good enough as it does not give us a crisp understanding. At the same time, we cannot afford not to use deep learning, namely, the optimization of parametric functions (neural nets) by means of stochastic gradient descent, because <strong>deep learning represents a very useful, versatile, and effective class of solvers.</strong></p></blockquote>
<h3 id="an-intro-icaps-2024-keynote-hector-geffner-on-learning-representations-to-act-and-plan">An intro: ICAPS 2024 Keynote: Hector Geffner on “Learning Representations to Act and Plan”<a aria-hidden="true" class="anchor" hidden="" href="#an-intro-icaps-2024-keynote-hector-geffner-on-learning-representations-to-act-and-plan">#</a></h3>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube-nocookie.com/embed/LZyKfCB4IAk?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"></iframe>
</div>
<h3 id="learning-sketch-decompositions-in-planning-via-deep-reinforcement-learning">Learning Sketch Decompositions in Planning via Deep Reinforcement Learning<a aria-hidden="true" class="anchor" hidden="" href="#learning-sketch-decompositions-in-planning-via-deep-reinforcement-learning">#</a></h3>
<p><a href="https://arxiv.org/abs/2412.08574">https://arxiv.org/abs/2412.08574</a></p>
<p><img alt="image-20250304123936199" loading="lazy" src="/posts/learning_general_policy_by_hector_2025/image-assets/image-20250304123936199.png"/></p>
<blockquote>
<p>Conclusion quote: We have demonstrated that DRL methods can effectively learn the common subgoal structures of entire collections of planning problems, enabling them to be solved efficiently through greedy sequences of IW(k) searches.</p></blockquote>
<h3 id="symmetries-and-expressive-requirements-for-learning-general-policies">Symmetries and Expressive Requirements for Learning General Policies<a aria-hidden="true" class="anchor" hidden="" href="#symmetries-and-expressive-requirements-for-learning-general-policies">#</a></h3>
<p><a href="https://arxiv.org/pdf/2409.15892">https://arxiv.org/pdf/2409.15892</a></p>
<blockquote>
<p>Abstract: State symmetries play an important role in planning and generalized planning. In the first case, state symmetries can be used to reduce the size of the search; in the second, to reduce the size of the training set. In the case of general planning, however, it is also critical to distinguish non-symmetric states, i.e., states that represent non-isomorphic relational structures. However, while the language of first-order logic distinguishes non-symmetric states, the languages and architectures used to represent and learn general policies do not. In particular, recent approaches for learning general policies use state features derived from description logics or learned via graph neural networks (GNNs) that are known to be limited by the expressive power of C2, first-order logic with two variables and counting. In this work, we address the problem of detecting symmetries in planning and generalized planning and use the results to assess the expressive requirements for learning general policies over various planning domains. <strong>For this, we map planning states to plain graphs, run off-the-shelf</strong>
<strong>algorithms to determine whether two states are isomorphic with respect to the goal, and run coloring algorithms to determine if C2 features computed logically or via GNNs distinguish non-isomorphic states.</strong> Symmetry detection results in more effective learning, while the failure to detect non-symmetries prevents general policies from being learned at all in certain domains</p></blockquote>
<h3 id="on-policy-reuse-an-expressive-language-for-representing-and-executing-general-policies-that-call-other-policies">On Policy Reuse: An Expressive Language for Representing and Executing General Policies that Call Other Policies<a aria-hidden="true" class="anchor" hidden="" href="#on-policy-reuse-an-expressive-language-for-representing-and-executing-general-policies-that-call-other-policies">#</a></h3>
<p><a href="https://openreview.net/forum?id=TSl0tWPiXT">https://openreview.net/forum?id=TSl0tWPiXT</a></p>
<p><a href="https://ojs.aaai.org/index.php/ICAPS/article/view/31458/33618">https://ojs.aaai.org/index.php/ICAPS/article/view/31458/33618</a></p>
<p><img alt="image-20250304125002624" loading="lazy" src="/posts/learning_general_policy_by_hector_2025/image-assets/image-20250304125002624.png"/></p>
<p>It provides a types of language to express the general policies (which is a collection of rules)</p>
<h3 id="learning-general-policies-with-policy-gradient-methods">Learning General Policies with Policy Gradient Methods<a aria-hidden="true" class="anchor" hidden="" href="#learning-general-policies-with-policy-gradient-methods">#</a></h3>
<p><a href="https://proceedings.kr.org/2023/63/kr2023-0063-stahlberg-et-al.pdf">https://proceedings.kr.org/2023/63/kr2023-0063-stahlberg-et-al.pdf</a></p>
<p>The learning of general policy is different from standard policy gradient learning as the general policy has different semantic than the standard policy</p>
<p><img alt="image-20250304125516046" loading="lazy" src="/posts/learning_general_policy_by_hector_2025/image-assets/image-20250304125516046.png"/></p>
<h3 id="the-long-dicussion-of-the-search">The long dicussion of the search: <a href="https://arxiv.org/pdf/2311.05490">https://arxiv.org/pdf/2311.05490</a><a aria-hidden="true" class="anchor" hidden="" href="#the-long-dicussion-of-the-search">#</a></h3>
<p><img alt="image-20250304125656572" loading="lazy" src="/posts/learning_general_policy_by_hector_2025/image-assets/image-20250304125656572.png"/></p>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/knowledge-representation/">Knowledge Representation</a></li>
<li><a href="https://sino-huang.github.io/tags/sketch/">Sketch</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/htn_planning_by_pascal_2025/">
<span class="title">« Prev</span>
<br/>
<span>HTN planning @ Pascal Bercher ANU</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/shenan-model-based-rpgm-2023/">
<span class="title">Next »</span>
<br/>
<span>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms 2023</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on x" href="https://x.com/intent/tweet/?text=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f&amp;hashtags=knowledgerepresentation%2csketch" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f&amp;title=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner&amp;summary=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f&amp;title=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on whatsapp" href="https://api.whatsapp.com/send?text=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on telegram" href="https://telegram.me/share/url?text=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share Learning General Policies Through Sketch @ Hector Geffner on ycombinator" href="https://news.ycombinator.com/submitlink?t=Learning%20General%20Policies%20Through%20Sketch%20%40%20Hector%20Geffner&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2flearning_general_policy_by_hector_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
