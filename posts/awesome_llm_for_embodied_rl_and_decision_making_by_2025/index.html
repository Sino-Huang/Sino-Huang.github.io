<!DOCTYPE html>
<html dir="auto" lang="en">
<head><meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<meta content="index, follow" name="robots"/>
<title>VLM/LLM for Embodied Agents, LLMs working as part of the policy | Sukai Huang</title>
<meta content="llm as policy model in online RL setting" name="keywords"/>
<meta content="The study in this field is very messy I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it instruction following with LM, some people call it language grounding in embodied environments, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other’s work and compare with each other. So, I want to say be careful to enter this area." name="description"/>
<meta content="Sukai Huang" name="author"/>
<link href="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/" rel="canonical"/>
<meta content="IFgzhtDTVCjONQMwQsBfuf0ZyHdzUR5WFYzbWsf2Gf8" name="google-site-verification"/>
<link as="style" crossorigin="anonymous" href="/assets/css/stylesheet.216db197d28df88a47bfb89498be748381c9d960a20db4b7d59e51e8b91f60b6.css" integrity="sha256-IW2xl9KN+IpHv7iUmL50g4HJ2WCiDbS31Z5R6LkfYLY=" rel="preload stylesheet"/>
<link href="https://sino-huang.github.io/favicon.ico" rel="icon"/>
<link href="https://sino-huang.github.io/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://sino-huang.github.io/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://sino-huang.github.io/apple-touch-icon.png" rel="apple-touch-icon"/>
<link href="https://sino-huang.github.io/safari-pinned-tab.svg" rel="mask-icon"/>
<meta content="#2e2e33" name="theme-color"/>
<meta content="#2e2e33" name="msapplication-TileColor"/>
<link href="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/" hreflang="en" rel="alternate"/>
<noscript>
<style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
<style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css" integrity="sha384-veTAhWILPOotXm+kbR5uY7dRamYLJf58I7P+hJhjeuc7hsMAkJHTsPahAl0hBST0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-v6mkHYHfY/4BWq54f7lQAdtIsoZZIByznQ3ZqN38OL4KCsrxo31SLlPiak7cj/Mg" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TTFTV1EWH5"></script>
<script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-TTFTV1EWH5');
        }
      </script><meta content="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/" property="og:url"/>
<meta content="Sukai Huang" property="og:site_name"/>
<meta content="VLM/LLM for Embodied Agents, LLMs working as part of the policy" property="og:title"/>
<meta content="The study in this field is very messy I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it instruction following with LM, some people call it language grounding in embodied environments, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other’s work and compare with each other. So, I want to say be careful to enter this area." property="og:description"/>
<meta content="en" property="og:locale"/>
<meta content="article" property="og:type"/>
<meta content="posts" property="article:section"/>
<meta content="2025-03-01T21:49:37+11:00" property="article:published_time"/>
<meta content="2025-03-01T21:49:37+11:00" property="article:modified_time"/>
<meta content="Llm as Policy Model in Online RL Setting" property="article:tag"/>
<meta content="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/cover.png" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/cover.png" name="twitter:image"/>
<meta content="VLM/LLM for Embodied Agents, LLMs working as part of the policy" name="twitter:title"/>
<meta content="The study in this field is very messy I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it instruction following with LM, some people call it language grounding in embodied environments, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other’s work and compare with each other. So, I want to say be careful to enter this area." name="twitter:description"/>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://sino-huang.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VLM/LLM for Embodied Agents, LLMs working as part of the policy",
      "item": "https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VLM/LLM for Embodied Agents, LLMs working as part of the policy",
  "name": "VLM\/LLM for Embodied Agents, LLMs working as part of the policy",
  "description": "The study in this field is very messy I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it instruction following with LM, some people call it language grounding in embodied environments, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other\u0026rsquo;s work and compare with each other. So, I want to say be careful to enter this area.\n",
  "keywords": [
    "llm as policy model in online RL setting"
  ],
  "articleBody": "The study in this field is very messy I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it instruction following with LM, some people call it language grounding in embodied environments, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other’s work and compare with each other. So, I want to say be careful to enter this area.\nSurvey paper: A Survey of Reinforcement Learning Informed by Natural Language https://arxiv.org/pdf/1906.03926\nTRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH EMBODIED ENVIRONMENTS VIA REINFORCEMENT LEARNING https://arxiv.org/pdf/2401.14151\nLARGE LANGUAGE MODELS AS GENERALIZABLE POLICIES FOR EMBODIED TASKS https://arxiv.org/pdf/2310.17722\nTo our knowledge, no prior work demonstrates that LLMs can be used as vision-language policies in online RL problems to improve generalization.\nThat adapter layer is also used in Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. ICML 2023: 26311-26325\nThis is the statement made by the authors.\nThe reviewer call it as adapting large language models to embodied visual tasks - particularly in online reinforcement learning setting\nhttps://openreview.net/forum?id=u6imHU4Ebu\nGrounding Large Language Models in Interactive Environments with Online Reinforcement Learning https://arxiv.org/pdf/2302.02662v4\nLearning to Model the World With Language * https://arxiv.org/pdf/2308.01399\nALFWORLD: ALIGNING TEXT AND EMBODIED ENVIRONMENTS FOR INTERACTIVE LEARNING https://arxiv.org/pdf/2010.03768\nHuman Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text https://arxiv.org/pdf/2005.09382\nReflect-RL: Two-Player Online RL Fine-Tuning for LMs https://arxiv.org/pdf/2402.12621\nDescribe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents https://arxiv.org/pdf/2302.01560\nNatural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation https://arxiv.org/pdf/2302.09368\nDo Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling https://arxiv.org/abs/2301.12050\nThis also belongs to Plan first and then train policy but this time it is modular policy.\nICML 2023\nA nice thing for their policy model is that they use an adapter to the original decision transformer policy model to train their own fine-tuned policy.\nRelevant but in Robotics field, but they are using offline data to train low-level control policy and they are effective PaLM-E https://arxiv.org/abs/2303.03378 and SayCan https://say-can.github.io/assets/palm_saycan.pdf\nSayCan, CodeAsPolicy and ProgPrompt are zero-shot applications of LLMs that require describing the environment in text\nPaLM-E are offline: they are given 130,000 human teleoperated demonstrations collected over almost 2 years as training data [2,3]\nBoth try to separate into high-level plan action and then low-level control action\nPalm-E\nSaycan\n",
  "wordCount" : "444",
  "inLanguage": "en",
  "image":"https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/cover.png","datePublished": "2025-03-01T21:49:37+11:00",
  "dateModified": "2025-03-01T21:49:37+11:00",
  "author":{
    "@type": "Person",
    "name": "Sukai Huang"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Sukai Huang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://sino-huang.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<header class="header">
<nav class="nav">
<div class="logo">
<a accesskey="h" href="https://sino-huang.github.io/" title="Sukai Huang (Alt + H)">Sukai Huang</a>
<div class="logo-switches">
<button accesskey="t" id="theme-toggle" title="(Alt + T)">
<svg fill="none" height="18" id="moon" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
</svg>
<svg fill="none" height="18" id="sun" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewbox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
<circle cx="12" cy="12" r="5"></circle>
<line x1="12" x2="12" y1="1" y2="3"></line>
<line x1="12" x2="12" y1="21" y2="23"></line>
<line x1="4.22" x2="5.64" y1="4.22" y2="5.64"></line>
<line x1="18.36" x2="19.78" y1="18.36" y2="19.78"></line>
<line x1="1" x2="3" y1="12" y2="12"></line>
<line x1="21" x2="23" y1="12" y2="12"></line>
<line x1="4.22" x2="5.64" y1="19.78" y2="18.36"></line>
<line x1="18.36" x2="19.78" y1="5.64" y2="4.22"></line>
</svg>
</button>
</div>
</div>
<ul id="menu">
<li>
<a href="https://sino-huang.github.io/biography/" title="Biography">
<span>Biography</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/archives" title="Archive">
<span>Archive</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/tags/" title="Tags">
<span>Tags</span>
</a>
</li>
<li>
<a href="https://sino-huang.github.io/categories/" title="Categories">
<span>Categories</span>
</a>
</li>
<li>
<a accesskey="/" href="https://sino-huang.github.io/search/" title="Search (Alt + /)">
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class="main">
<article class="post-single">
<header class="post-header">
<div class="breadcrumbs"><a href="https://sino-huang.github.io/">Home</a> » <a href="https://sino-huang.github.io/posts/">Posts</a></div>
<h1 class="post-title entry-hint-parent">
      VLM/LLM for Embodied Agents, LLMs working as part of the policy
    </h1>
<div class="post-meta"><span title="2025-03-01 21:49:37 +1100 AEDT">March 1, 2025</span> · 3 min · 444 words · Sukai Huang | <a href="mailto:sukaih@student.unimelb.edu.au/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/index.md" rel="noopener noreferrer" target="_blank">Submit a report</a>
</div>
</header>
<figure class="entry-cover"><img alt="" loading="eager" src="https://sino-huang.github.io/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/cover.png"/>
<p><text></text></p>
</figure><div class="toc">
<details>
<summary accesskey="c" title="(Alt + C)">
<span class="details">Table of Contents</span>
</summary>
<div class="inner"><ul><ul>
<li>
<a aria-label="Survey paper: A Survey of Reinforcement Learning Informed by Natural Language" href="#survey-paper-a-survey-of-reinforcement-learning-informed-by-natural-language">Survey paper: A Survey of Reinforcement Learning Informed by Natural Language</a></li>
<li>
<a aria-label="TRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH EMBODIED ENVIRONMENTS VIA REINFORCEMENT LEARNING" href="#true-knowledge-comes-from-practice-aligning-llms-with-embodied-environments-via-reinforcement-learning">TRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH EMBODIED ENVIRONMENTS VIA REINFORCEMENT LEARNING</a></li>
<li>
<a aria-label="LARGE LANGUAGE MODELS AS GENERALIZABLE POLICIES FOR EMBODIED TASKS" href="#large-language-models-as-generalizable-policies-for-embodied-tasks">LARGE LANGUAGE MODELS AS GENERALIZABLE POLICIES FOR EMBODIED TASKS</a></li>
<li>
<a aria-label="Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning" href="#grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning">Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</a></li>
<li>
<a aria-label="Learning to Model the World With Language *" href="#learning-to-model-the-world-with-language-">Learning to Model the World With Language *</a></li>
<li>
<a aria-label="ALFWORLD: ALIGNING TEXT AND EMBODIED ENVIRONMENTS FOR INTERACTIVE LEARNING" href="#alfworld-aligning-text-and-embodied-environments-for-interactive-learning">ALFWORLD: ALIGNING TEXT AND EMBODIED ENVIRONMENTS FOR INTERACTIVE LEARNING</a></li>
<li>
<a aria-label="Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text" href="#human-instruction-following-with-deep-reinforcement-learning-via-transfer-learning-from-text">Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text</a></li>
<li>
<a aria-label="Reflect-RL: Two-Player Online RL Fine-Tuning for LMs" href="#reflect-rl-two-player-online-rl-fine-tuning-for-lms">Reflect-RL: Two-Player Online RL Fine-Tuning for LMs</a></li>
<li>
<a aria-label="Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents" href="#describe-explain-plan-and-select-interactive-planning-with-large-language-models-enables-open-world-multi-task-agents">Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents</a></li>
<li>
<a aria-label="Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation" href="#natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation">Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation</a></li>
<li>
<a aria-label="Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling" href="#do-embodied-agents-dream-of-pixelated-sheep-embodied-decision-making-using-language-guided-world-modelling">Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling</a></li></ul>
<li>
<a aria-label="Relevant but in Robotics field, but they are using offline data to train low-level control policy and they are effective" href="#relevant-but-in-robotics-field-but-they-are-using-offline-data-to-train-low-level-control-policy-and-they-are-effective">Relevant but in Robotics field, but they are using offline data to train low-level control policy and they are effective</a>
</li>
</ul>
</div>
</details>
</div>
<div class="post-content"><p>The study in this field is very <em><strong>messy</strong></em> I should say, a lot of researchers coming from different background and most of them try to publish their own embodied environments and baseline models. There is a lack of systematic study in this field. Most importantly, their model are really difficult to reproduce. In fact, there is no standard phrase for this research field. Some people call it <strong>instruction following with LM</strong>, some people call it <strong>language grounding in embodied environments</strong>, some people call it instruction-following with RL and all the papers in this area did not even try to reproduce other’s work and compare with each other. So, I want to say be careful to enter this area.</p>
<h3 id="survey-paper-a-survey-of-reinforcement-learning-informed-by-natural-language">Survey paper: A Survey of Reinforcement Learning Informed by Natural Language<a aria-hidden="true" class="anchor" hidden="" href="#survey-paper-a-survey-of-reinforcement-learning-informed-by-natural-language">#</a></h3>
<p><a href="https://arxiv.org/pdf/1906.03926">https://arxiv.org/pdf/1906.03926</a></p>
<p><img alt="image-20250301220023340" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301220023340.png"/></p>
<h3 id="true-knowledge-comes-from-practice-aligning-llms-with-embodied-environments-via-reinforcement-learning">TRUE KNOWLEDGE COMES FROM PRACTICE: ALIGNING LLMS WITH EMBODIED ENVIRONMENTS VIA REINFORCEMENT LEARNING<a aria-hidden="true" class="anchor" hidden="" href="#true-knowledge-comes-from-practice-aligning-llms-with-embodied-environments-via-reinforcement-learning">#</a></h3>
<p><a href="https://arxiv.org/pdf/2401.14151">https://arxiv.org/pdf/2401.14151</a></p>
<p><img alt="image-20250301215955243" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301215955243.png"/></p>
<h3 id="large-language-models-as-generalizable-policies-for-embodied-tasks">LARGE LANGUAGE MODELS AS GENERALIZABLE POLICIES FOR EMBODIED TASKS<a aria-hidden="true" class="anchor" hidden="" href="#large-language-models-as-generalizable-policies-for-embodied-tasks">#</a></h3>
<p><a href="https://arxiv.org/pdf/2310.17722">https://arxiv.org/pdf/2310.17722</a></p>
<p><img alt="image-20250301220331815" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301220331815.png"/></p>
<blockquote>
<p>To our knowledge, no prior work demonstrates that LLMs can be used as vision-language policies in online RL problems to improve generalization.</p>
</blockquote>
<p>That adapter layer is also used in <code>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling. ICML 2023: 26311-26325</code></p>
<p>This is the statement made by the authors.</p>
<p>The reviewer call it as <em>adapting large language models to embodied visual tasks - particularly in online reinforcement learning setting</em></p>
<p><a href="https://openreview.net/forum?id=u6imHU4Ebu">https://openreview.net/forum?id=u6imHU4Ebu</a></p>
<h3 id="grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning">Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning<a aria-hidden="true" class="anchor" hidden="" href="#grounding-large-language-models-in-interactive-environments-with-online-reinforcement-learning">#</a></h3>
<p><a href="https://arxiv.org/pdf/2302.02662v4">https://arxiv.org/pdf/2302.02662v4</a></p>
<p><img alt="image-20250301221722208" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301221722208.png"/></p>
<h3 id="learning-to-model-the-world-with-language-">Learning to Model the World With Language *<a aria-hidden="true" class="anchor" hidden="" href="#learning-to-model-the-world-with-language-">#</a></h3>
<p><a href="https://arxiv.org/pdf/2308.01399">https://arxiv.org/pdf/2308.01399</a></p>
<p><img alt="image-20250301221753069" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301221753069.png"/></p>
<h3 id="alfworld-aligning-text-and-embodied-environments-for-interactive-learning">ALFWORLD: ALIGNING TEXT AND EMBODIED ENVIRONMENTS FOR INTERACTIVE LEARNING<a aria-hidden="true" class="anchor" hidden="" href="#alfworld-aligning-text-and-embodied-environments-for-interactive-learning">#</a></h3>
<p><a href="https://arxiv.org/pdf/2010.03768">https://arxiv.org/pdf/2010.03768</a></p>
<p><img alt="image-20250301221930846" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301221930846.png"/></p>
<h3 id="human-instruction-following-with-deep-reinforcement-learning-via-transfer-learning-from-text">Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text<a aria-hidden="true" class="anchor" hidden="" href="#human-instruction-following-with-deep-reinforcement-learning-via-transfer-learning-from-text">#</a></h3>
<p><a href="https://arxiv.org/pdf/2005.09382">https://arxiv.org/pdf/2005.09382</a></p>
<p><img alt="image-20250301222014724" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301222014724.png"/></p>
<h3 id="reflect-rl-two-player-online-rl-fine-tuning-for-lms">Reflect-RL: Two-Player Online RL Fine-Tuning for LMs<a aria-hidden="true" class="anchor" hidden="" href="#reflect-rl-two-player-online-rl-fine-tuning-for-lms">#</a></h3>
<p><a href="https://arxiv.org/pdf/2402.12621">https://arxiv.org/pdf/2402.12621</a></p>
<p><img alt="image-20250301222125653" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301222125653.png"/></p>
<h3 id="describe-explain-plan-and-select-interactive-planning-with-large-language-models-enables-open-world-multi-task-agents">Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents<a aria-hidden="true" class="anchor" hidden="" href="#describe-explain-plan-and-select-interactive-planning-with-large-language-models-enables-open-world-multi-task-agents">#</a></h3>
<p><a href="https://arxiv.org/pdf/2302.01560">https://arxiv.org/pdf/2302.01560</a></p>
<p><img alt="image-20250301222226810" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301222226810.png"/></p>
<h3 id="natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation">Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation<a aria-hidden="true" class="anchor" hidden="" href="#natural-language-conditioned-reinforcement-learning-with-inside-out-task-language-development-and-translation">#</a></h3>
<p><a href="https://arxiv.org/pdf/2302.09368">https://arxiv.org/pdf/2302.09368</a></p>
<p><img alt="image-20250301222247134" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250301222247134.png"/></p>
<h3 id="do-embodied-agents-dream-of-pixelated-sheep-embodied-decision-making-using-language-guided-world-modelling">Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling<a aria-hidden="true" class="anchor" hidden="" href="#do-embodied-agents-dream-of-pixelated-sheep-embodied-decision-making-using-language-guided-world-modelling">#</a></h3>
<p><a href="https://arxiv.org/abs/2301.12050">https://arxiv.org/abs/2301.12050</a></p>
<p>This also belongs to Plan first and then train policy but this time it is <strong>modular policy.</strong></p>
<p>ICML 2023</p>
<p><img alt="image-20250310230052239" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250310230052239.png"/></p>
<p>A nice thing for their policy model is that they use an adapter to the original decision transformer policy model to train their own fine-tuned policy.</p>
<p><img alt="image-20250310230411651" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250310230411651.png"/></p>
<h2 id="relevant-but-in-robotics-field-but-they-are-using-offline-data-to-train-low-level-control-policy-and-they-are-effective">Relevant but in Robotics field, but they are using offline data to train low-level control policy and they are effective<a aria-hidden="true" class="anchor" hidden="" href="#relevant-but-in-robotics-field-but-they-are-using-offline-data-to-train-low-level-control-policy-and-they-are-effective">#</a></h2>
<p>PaLM-E <a href="https://arxiv.org/abs/2303.03378">https://arxiv.org/abs/2303.03378</a> and SayCan <a href="https://say-can.github.io/assets/palm_saycan.pdf">https://say-can.github.io/assets/palm_saycan.pdf</a></p>
<p>SayCan, CodeAsPolicy and ProgPrompt are zero-shot applications of LLMs that require describing the environment in text</p>
<p>PaLM-E are offline:  they are given 130,000 human teleoperated demonstrations collected over almost 2 years as training data [2,3]</p>
<p>Both try to separate into high-level plan action and then low-level control action</p>
<p><strong>Palm-E</strong></p>
<p><img alt="image-20250310232641615" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250310232641615.png"/></p>
<p><strong>Saycan</strong></p>
<p><img alt="image-20250310232719880" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250310232719880.png"/></p>
<p><img alt="image-20250310232854916" loading="lazy" src="/posts/awesome_llm_for_embodied_rl_and_decision_making_by_2025/image-assets/image-20250310232854916.png"/></p>
</div>
<footer class="post-footer">
<ul class="post-tags">
<li><a href="https://sino-huang.github.io/tags/llm-as-policy-model-in-online-rl-setting/">Llm as Policy Model in Online RL Setting</a></li>
</ul>
<nav class="paginav">
<a class="prev" href="https://sino-huang.github.io/posts/shenan-model-based-rpgm-2023/">
<span class="title">« Prev</span>
<br/>
<span>Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms 2023</span>
</a>
<a class="next" href="https://sino-huang.github.io/posts/neuro_symbolic_works_from_hamid_2024_2025/">
<span class="title">Next »</span>
<br/>
<span>Neuro Symbolic Works From A.Prof. Hamid @ Monash</span>
</a>
</nav>
<ul class="share-buttons">
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on x" href="https://x.com/intent/tweet/?text=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f&amp;hashtags=llmaspolicymodelinonlineRLsetting" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f&amp;title=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy&amp;summary=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy&amp;source=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f&amp;title=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on whatsapp" href="https://api.whatsapp.com/send?text=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy%20-%20https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve">
<path d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on telegram" href="https://telegram.me/share/url?text=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy&amp;url=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="2 2 28 28" width="30px" xml:space="preserve">
<path d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z"></path>
</svg>
</a>
</li>
<li>
<a aria-label="share VLM/LLM for Embodied Agents, LLMs working as part of the policy on ycombinator" href="https://news.ycombinator.com/submitlink?t=VLM%2fLLM%20for%20Embodied%20Agents%2c%20LLMs%20working%20as%20part%20of%20the%20policy&amp;u=https%3a%2f%2fsino-huang.github.io%2fposts%2fawesome_llm_for_embodied_rl_and_decision_making_by_2025%2f" rel="noopener noreferrer" target="_blank">
<svg fill="currentColor" height="30px" version="1.1" viewbox="0 0 512 512" width="30px" xml:space="preserve" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
<path d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z"></path>
</svg>
</a>
</li>
</ul>
</footer>
</article>
</main>
<footer class="footer">
<span>© 2025 <a href="https://sino-huang.github.io/">Sukai Huang</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &amp;
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
</span>
</footer>
<a accesskey="g" aria-label="go to top" class="top-link" href="#top" id="top-link" title="Go to Top (Alt + G)">
<svg fill="currentColor" viewbox="0 0 12 6" xmlns="http://www.w3.org/2000/svg">
<path d="M12 6H0l6-6z"></path>
</svg>
</a>
<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
